{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mahjong Classification\n",
    "\n",
    "This notebook aim on aply transfer learning on vgg16 to Mahjong detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# import dependency\n",
    "\n",
    "import os\n",
    "import math\n",
    "import h5py\n",
    "import numpy as np\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import optimizers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution2D, MaxPooling2D, ZeroPadding2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import TensorBoard, LearningRateScheduler\n",
    "import keras\n",
    "import datetime\n",
    "import time\n",
    "import sys\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# dimensions of our images.\n",
    "#img_width, img_height = 224, 224\n",
    "#as most of our test pattern around 40*40 , lets try some smaller size\n",
    "#reset as no batter performace in 43 classes\n",
    "img_width, img_height = 80, 80 \n",
    "\n",
    "train_data_dir = 'data/train'\n",
    "validation_data_dir = 'data/validation'\n",
    "test_data_dir = 'data/test/'\n",
    "nb_train_samples = 10800\n",
    "nb_validation_samples = 450\n",
    "#nb_test_samples = 550\n",
    "\n",
    "#nb_epoch = 100\n",
    "#it seems it cound learn more\n",
    "nb_epoch = 35\n",
    "#nb_class = 34\n",
    "# update for classify 42 pattern and backOnly\n",
    "nb_class = 43\n",
    "\n",
    "ts = time.time()\n",
    "logName = datetime.datetime.fromtimestamp(ts).strftime('%Y_%m_%d_%H_%M_%S')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### records:\n",
    "|name| abort reason|\n",
    "|----|-----|\n",
    "| 2017_03_09_14_17_47 | change code to take more number of samples |\n",
    "| 2017_03_09_14_51_31 | computer needed to shutdown |\n",
    "\n",
    "|graph name |settings remarks| img dimensions| nb train samples | nb validation samples | nb_epoch | num of classes|img color info|\n",
    "|-----------|----------------|---------------|------------------|-----------------------|----------|---------------|------|\n",
    "|2017_03_08_12_42_19  | no change  | 224x224  | 2000 | 100| 100| 43| RGB|\n",
    "|2017_03_08_16_42_42  | remove BackOnly Class  | 224x224  | 2000 | 100 | 100 | 42 | RGB |\n",
    "|2017_03_08_21_34_23  | gray scale of 1st train | 224x224  | 2000 | 100 | 100 | 43 | gray |\n",
    "|2017_03_09_02_50_02  | gray scale of 2nd train | 224x224  | 2000 | 100 | 100 | 42 | gray |\n",
    "|2017_03_09_11_46_43  | lower dimensions of 42 classes | 112x112 | 2000 | 100 | 100 | 42 | gray |\n",
    "|2017_03_09_13_11_36  | lower dimensions of 43 classes | 112x112 | 2000 | 100 | 100 | 43 | gray |\n",
    "|2017_03_09_14_17_47  | increase number of steps (aborted ) | 224x224 | 2000 | 100 | 150 | 43 | gray |\n",
    "|2017_03_09_14_51_31  | fix the number of train data (aborted ) | 224x224 | 10800| 450 | 100 | 43 | gray |\n",
    "|2017_03_10_22_47_14  | fine tune parameters base on above results | 80x80 | 10800 | 450 | 35 |43 | gray |\n",
    "|2017_03_12_02_21_50  | try to print evaluation in each epoch (failed )| 80x80 | 10800 | 450 | 35 |43 | gray |\n",
    "|  | try to print evaluation at tensorboardd in each epoch (failed )| 80x80 | 10800 | 450 | 35 |43 | gray |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DO NOT change the code, just add block\n",
    "as cannot reverse\n",
    "\n",
    "\n",
    "weights= None Or imagenet -- use the pretrained info\n",
    "include_top=False -- do not use the trained relationship between class and feature\n",
    "Dropout -- persent that drop some sample such that the nodel would not overfeed\n",
    "softmax -- find the probability for classification\n",
    "-----------\n",
    "adadelta-- the best method to find the global min of the loss for this situation\n",
    "metrics=['accuracy']--the tensorbox will log the accuracy\n",
    "------------\n",
    "train_datagen -- param for generating imgs\n",
    "--------\n",
    "lrate -- learning rate\n",
    "-----\n",
    "nb_epoch -- number of steps\n",
    "----\n",
    "observation:\n",
    "43 Vs 42 classes --> higher initial acc in 43 (as removed inbalaced a class, backOnly with high amount of data)\n",
    "224 * 224 Vs 112 * 112 img size ???? performace batter in 42 classes, but worse in 43 classes\n",
    "-----\n",
    "faster R-CNN-- draw the bounding box-- paper-- std method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# build the VGG16 network 'imagenet'\n",
    "model = Sequential()\n",
    "vggLayer = VGG16(include_top=False, weights=None, input_shape=(img_width, img_height, 3))\n",
    "\n",
    "\n",
    "model.add(vggLayer)\n",
    "model.add(Flatten(input_shape=model.output_shape[1:]))\n",
    "model.add(Dense(2000, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(output_dim=nb_class, activation='softmax', name='fc2'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2325 images belonging to 43 classes.\n"
     ]
    }
   ],
   "source": [
    "#load the weight \n",
    "# load weights into new model\n",
    "model.load_weights(\"weights/2017_03_10_22_47_14/weights-improvement-20-0.95.hdf5\")\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adadelta',\n",
    "              metrics=['accuracy'])\n",
    "\"\"\"\n",
    "# prepare data augmentation configuration\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True)\n",
    "\"\"\"\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\"\"\"\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        train_data_dir,\n",
    "        target_size=(img_height, img_width),\n",
    "        batch_size=8,\n",
    "        class_mode='categorical')\n",
    "\"\"\"\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "        validation_data_dir,\n",
    "        target_size=(img_height, img_width),\n",
    "        batch_size=8,\n",
    "        class_mode='categorical')\n",
    "\n",
    "\n",
    "\n",
    "# learning rate schedule\n",
    "def step_decay(epoch):\n",
    "    initial_lrate = 0.25\n",
    "    drop = 0.25\n",
    "    epochs_drop = 20.0\n",
    "    lrate = initial_lrate * math.pow(drop, math.floor((1+epoch)/epochs_drop))\n",
    "    return lrate\n",
    "\n",
    "lrate = LearningRateScheduler(step_decay)\n",
    "\n",
    "tfBoard = keras.callbacks.TensorBoard(log_dir='./logs/'+logName+\"/\", histogram_freq=2, write_graph=True)\n",
    "filepath=\"./weights/\"+logName+\"/weights-improvement-{epoch:02d}-{val_acc:.2f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "\n",
    "callbacks_list = [checkpoint, tfBoard, lrate]\n",
    "\n",
    "\n",
    "if not os.path.exists(\"./weights/\"):\n",
    "    os.makedirs(\"./weights/\")   \n",
    "if not os.path.exists('./logs/'):\n",
    "    os.makedirs('./logs/')   \n",
    "if not os.path.exists(\"./weights/\"+logName):\n",
    "    os.makedirs(\"./weights/\"+logName)   \n",
    "if not os.path.exists('./logs/'+logName):\n",
    "    os.makedirs('./logs/'+logName)   \n",
    "#if not os.path.exists('./logs/'+logName+\"_test\"):\n",
    "#    os.makedirs('./logs/'+logName+\"_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 591 images belonging to 43 classes.\n",
      "loss: 29.67%\n",
      " acc: 94.08%\n"
     ]
    }
   ],
   "source": [
    "#code for printing the prediciton \n",
    "#predict_generator(self, generator, val_samples, max_q_size=10, nb_worker=1, pickle_safe=False)\n",
    "\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "        test_data_dir,\n",
    "        target_size=(img_height, img_width),\n",
    "        batch_size=8,\n",
    "        class_mode='categorical',\n",
    "        shuffle=False\n",
    "        )\n",
    "\n",
    "\n",
    "predicted = model.predict_generator(test_generator, val_samples=591)\n",
    "\n",
    "test_pos_class_dict = dict(zip(test_generator.class_indices.values(), test_generator.class_indices.keys()) )\n",
    "\n",
    "\n",
    "with open(\"./predictedResult.csv\",'a') as predictedCsv:\n",
    "    predictedCsv.write( \"fileName,Org Class,Predicted Class\\n\" )\n",
    "    for idx, _ in enumerate(predicted):\n",
    "        predictedCsv.write(  str(os.path.basename(imgfilenames[idx])) +\",\"+ str(os.path.dirname(imgfilenames[idx]))+\",\"+ str(test_pos_class_dict.get( predicted[idx].argmax()))+'\\n')\n",
    "\n",
    "\n",
    "score = model.evaluate_generator(test_generator, val_samples=591)\n",
    "print (\"loss: %.2f%%\\n acc: %.2f%%\" %( score[0]*100, score[1]*100) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name block1_conv1_W:0 is illegal; using block1_conv1_W_0 instead.\n",
      "INFO:tensorflow:Summary name block1_conv1_b:0 is illegal; using block1_conv1_b_0 instead.\n",
      "INFO:tensorflow:Summary name block1_conv2_W:0 is illegal; using block1_conv2_W_0 instead.\n",
      "INFO:tensorflow:Summary name block1_conv2_b:0 is illegal; using block1_conv2_b_0 instead.\n",
      "INFO:tensorflow:Summary name block2_conv1_W:0 is illegal; using block2_conv1_W_0 instead.\n",
      "INFO:tensorflow:Summary name block2_conv1_b:0 is illegal; using block2_conv1_b_0 instead.\n",
      "INFO:tensorflow:Summary name block2_conv2_W:0 is illegal; using block2_conv2_W_0 instead.\n",
      "INFO:tensorflow:Summary name block2_conv2_b:0 is illegal; using block2_conv2_b_0 instead.\n",
      "INFO:tensorflow:Summary name block3_conv1_W:0 is illegal; using block3_conv1_W_0 instead.\n",
      "INFO:tensorflow:Summary name block3_conv1_b:0 is illegal; using block3_conv1_b_0 instead.\n",
      "INFO:tensorflow:Summary name block3_conv2_W:0 is illegal; using block3_conv2_W_0 instead.\n",
      "INFO:tensorflow:Summary name block3_conv2_b:0 is illegal; using block3_conv2_b_0 instead.\n",
      "INFO:tensorflow:Summary name block3_conv3_W:0 is illegal; using block3_conv3_W_0 instead.\n",
      "INFO:tensorflow:Summary name block3_conv3_b:0 is illegal; using block3_conv3_b_0 instead.\n",
      "INFO:tensorflow:Summary name block4_conv1_W:0 is illegal; using block4_conv1_W_0 instead.\n",
      "INFO:tensorflow:Summary name block4_conv1_b:0 is illegal; using block4_conv1_b_0 instead.\n",
      "INFO:tensorflow:Summary name block4_conv2_W:0 is illegal; using block4_conv2_W_0 instead.\n",
      "INFO:tensorflow:Summary name block4_conv2_b:0 is illegal; using block4_conv2_b_0 instead.\n",
      "INFO:tensorflow:Summary name block4_conv3_W:0 is illegal; using block4_conv3_W_0 instead.\n",
      "INFO:tensorflow:Summary name block4_conv3_b:0 is illegal; using block4_conv3_b_0 instead.\n",
      "INFO:tensorflow:Summary name block5_conv1_W:0 is illegal; using block5_conv1_W_0 instead.\n",
      "INFO:tensorflow:Summary name block5_conv1_b:0 is illegal; using block5_conv1_b_0 instead.\n",
      "INFO:tensorflow:Summary name block5_conv2_W:0 is illegal; using block5_conv2_W_0 instead.\n",
      "INFO:tensorflow:Summary name block5_conv2_b:0 is illegal; using block5_conv2_b_0 instead.\n",
      "INFO:tensorflow:Summary name block5_conv3_W:0 is illegal; using block5_conv3_W_0 instead.\n",
      "INFO:tensorflow:Summary name block5_conv3_b:0 is illegal; using block5_conv3_b_0 instead.\n",
      "INFO:tensorflow:Summary name dense_1_W:0 is illegal; using dense_1_W_0 instead.\n",
      "INFO:tensorflow:Summary name dense_1_b:0 is illegal; using dense_1_b_0 instead.\n",
      "INFO:tensorflow:Summary name dense_2_W:0 is illegal; using dense_2_W_0 instead.\n",
      "INFO:tensorflow:Summary name dense_2_b:0 is illegal; using dense_2_b_0 instead.\n",
      "INFO:tensorflow:Summary name fc2_W:0 is illegal; using fc2_W_0 instead.\n",
      "INFO:tensorflow:Summary name fc2_b:0 is illegal; using fc2_b_0 instead.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "global name 'test_acc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-dd28f0c10dae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         nb_val_samples=nb_validation_samples)\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shawn/anaconda2/envs/openAi/lib/python2.7/site-packages/keras/models.pyc\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, samples_per_epoch, nb_epoch, verbose, callbacks, validation_data, nb_val_samples, class_weight, max_q_size, nb_worker, pickle_safe, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m    933\u001b[0m                                         \u001b[0mnb_worker\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnb_worker\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    934\u001b[0m                                         \u001b[0mpickle_safe\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpickle_safe\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 935\u001b[0;31m                                         initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m    936\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    937\u001b[0m     def evaluate_generator(self, generator, val_samples,\n",
      "\u001b[0;32m/home/shawn/anaconda2/envs/openAi/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, samples_per_epoch, nb_epoch, verbose, callbacks, validation_data, nb_val_samples, class_weight, max_q_size, nb_worker, pickle_safe, initial_epoch)\u001b[0m\n\u001b[1;32m   1514\u001b[0m             \u001b[0mcallback_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1515\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mnb_epoch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1516\u001b[0;31m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1517\u001b[0m                 \u001b[0msamples_seen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0mbatch_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shawn/anaconda2/envs/openAi/lib/python2.7/site-packages/keras/callbacks.pyc\u001b[0m in \u001b[0;36mon_epoch_begin\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m             \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delta_t_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delta_ts_batch_begin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeque\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shawn/anaconda2/envs/openAi/lib/python2.7/site-packages/keras/callbacks.pyc\u001b[0m in \u001b[0;36mon_epoch_begin\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m    528\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'lr'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Optimizer must have a \"lr\" attribute.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m         \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschedule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m             raise ValueError('The output of the \"schedule\" function '\n",
      "\u001b[0;32m<ipython-input-4-531dc2ad6028>\u001b[0m in \u001b[0;36mstep_decay\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0mtest_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics_names\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0mtest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics_names\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m     \u001b[0mtest_eva\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#add it for take log of test evaluation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mlrate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-531dc2ad6028>\u001b[0m in \u001b[0;36mtest_evaluate\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtest_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mtest_acc_sum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loss_sum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0mtest_writer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_acc_sum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mtest_writer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loss_sum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: global name 'test_acc' is not defined"
     ]
    }
   ],
   "source": [
    "# fine-tune the model\n",
    "model.fit_generator(\n",
    "        train_generator,\n",
    "        samples_per_epoch=nb_train_samples,\n",
    "        nb_epoch=nb_epoch,\n",
    "        callbacks=callbacks_list,\n",
    "        validation_data=validation_generator,\n",
    "        nb_val_samples=nb_validation_samples)\n",
    "\n",
    "\n",
    "\n",
    "model.save_weights(\"./weights/\"+logName+\"/final.hdf5\")\n",
    "\n",
    "print(\"please use tensorboard to view logs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "keras.__version__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
